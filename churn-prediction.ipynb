{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mbhosseini70/churn-prediction?scriptVersionId=146945751\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Customer churn prediction is a critical task for companies because of its challenges, as retaining existing customers is often more cost-effective than acquiring new ones. In this project, we will develop a churn prediction model using a dataset from a telecom company. The goal is to identify customers who are likely to churn, enabling the company to take proactive measures to retain them.\n\nThe project is divided into several steps\n\n* 1: Importing Libraries\n\nIn this step, we import the necessary Python libraries and modules for data analysis, visualization, machine learning, and model evaluation. We also suppress any warnings that may arise during the execution of the code.\n\n* 2: Loading Data\n\nWe load the dataset containing telecom customer information into a Pandas DataFrame. This dataset will serve as the foundation for building our churn prediction model.\n\n* 3: Exploratory Data Analysis and Visualization\n\nBefore preprocessing the data, we perform exploratory data analysis (EDA) to understand its characteristics. We check for missing values and address issues related to the \"TotalCharges\" column. We also visualize the distribution of tenures and monthly charges for both churned and non-churned customers using histograms.\n\n* 4: Data Preprocessing\n\nData preprocessing is a crucial step in preparing the data for machine learning. In this step, we define custom functions for handling categorical variables with binary values (\"Yes\" and \"No\"). We also create pipelines for preprocessing different types of features, such as binary (yes/no), ordinal (gender), categorical (contract, payment method, internet service), and numeric (tenure, monthly charges, total charges) features. These pipelines are combined into a single preprocessor pipeline.\n\n* 5: Model Selection and Grid Search\n\nWe define a function for grid search over various machine learning models to find the best hyperparameters. The models considered include Logistic Regression, Random Forest, XGBoost, CatBoost, and LightGBM. We use different scorers like recall, F1 score, and ROC AUC to evaluate model performance.\n\n* 6: Evaluation and Comparison and \n\nWe evaluate the models' performance on both the training and test datasets, calculating metrics such as accuracy, precision, recall, F1 score, and ROC AUC. We use a function to plot a comparison of these metrics across different models and scorers.\n\n* 7: Model Selection and Interpretation\n\nBased on the evaluation results, we select the CatBoostClassifier with optimized hyperparameters for AUC as our final model. We provide an explanation of the rationale behind this choice.\n\n* 8: Saving the Final Model\n\nWe save the selected CatBoostClassifier model to a file for future use.","metadata":{}},{"cell_type":"markdown","source":"# 1. Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport joblib\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import make_scorer, roc_auc_score, recall_score, f1_score\n\n\nfrom sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, OneHotEncoder, FunctionTransformer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n\nfrom tqdm.auto import tqdm\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-10-16T19:04:15.575646Z","iopub.execute_input":"2023-10-16T19:04:15.576054Z","iopub.status.idle":"2023-10-16T19:04:19.022567Z","shell.execute_reply.started":"2023-10-16T19:04:15.576023Z","shell.execute_reply":"2023-10-16T19:04:19.021364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Load data and make dataframe","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/it-customer-churn/IT_customer_churn.csv\")\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-10-16T19:04:19.024856Z","iopub.execute_input":"2023-10-16T19:04:19.025195Z","iopub.status.idle":"2023-10-16T19:04:19.118441Z","shell.execute_reply.started":"2023-10-16T19:04:19.025166Z","shell.execute_reply":"2023-10-16T19:04:19.117478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the distribution of the target variable 'Churn'\ndf['Churn'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-10-16T19:04:19.119736Z","iopub.execute_input":"2023-10-16T19:04:19.120088Z","iopub.status.idle":"2023-10-16T19:04:19.133191Z","shell.execute_reply.started":"2023-10-16T19:04:19.120063Z","shell.execute_reply":"2023-10-16T19:04:19.132076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Exploratory data analysis and Visualization","metadata":{}},{"cell_type":"code","source":"# Checking for missing values\ndf.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2023-10-16T19:04:19.13526Z","iopub.execute_input":"2023-10-16T19:04:19.135908Z","iopub.status.idle":"2023-10-16T19:04:19.148782Z","shell.execute_reply.started":"2023-10-16T19:04:19.135882Z","shell.execute_reply":"2023-10-16T19:04:19.147772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cheking  missing values in the 'TotalCharges' column\nprint(\"Number of Nan values in our dataset: \", df.isnull().sum().sum())\nprint(\"Number of Nan values when we convert TotalCharges to numeric: \", pd.to_numeric(df.TotalCharges,errors='coerce').isnull().sum())\nprint(\"Data where there is space (' ') in TotalCharges:\")\ndf[df[\"TotalCharges\"]==\" \"]","metadata":{"execution":{"iopub.status.busy":"2023-10-16T19:04:19.467302Z","iopub.execute_input":"2023-10-16T19:04:19.468186Z","iopub.status.idle":"2023-10-16T19:04:19.503518Z","shell.execute_reply.started":"2023-10-16T19:04:19.468144Z","shell.execute_reply":"2023-10-16T19:04:19.502748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing rows with missing 'TotalCharges' values\nprint(\"The dimension of the dataset before removing rows does not contain information on TotalCharges: \",  df.shape)\ndf = df[df[\"TotalCharges\"]!=\" \"]\ndf.TotalCharges = pd.to_numeric(df.TotalCharges)\nprint(\"The dimension of the dataset after removing rows does not contain information on TotalCharges: \",  df.shape)\n     ","metadata":{"execution":{"iopub.status.busy":"2023-10-16T19:04:19.95772Z","iopub.execute_input":"2023-10-16T19:04:19.958381Z","iopub.status.idle":"2023-10-16T19:04:19.973556Z","shell.execute_reply.started":"2023-10-16T19:04:19.958342Z","shell.execute_reply":"2023-10-16T19:04:19.972205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization: Creating histograms to compare tenure for Churn=No and Churn=Yes\n\n\n# Filter data\ntenure_churn_no = df[df.Churn == 'No'].tenure\ntenure_churn_yes = df[df.Churn == 'Yes'].tenure\n\n# Create histogram traces\ntrace_churn_no = go.Histogram(\n    x=tenure_churn_no,\n    opacity=0.5,\n    marker=dict(color='green'),\n    name='Churn=No',\n    nbinsx=10\n)\n\ntrace_churn_yes = go.Histogram(\n    x=tenure_churn_yes,\n    opacity=0.4,\n    marker=dict(color='red'),\n    name='Churn=Yes',\n    nbinsx=10\n)\n\n# Create layout\nlayout = go.Layout(\n    title=\"Customer Churn Prediction Visualization\",\n    xaxis=dict(title=\"Tenure\"),\n    yaxis=dict(title=\"Number Of Customers\"),\n    barmode='group',\n    bargap=0.4, \n    \n)\n\n# Create figure\nfig = go.Figure(data=[trace_churn_no, trace_churn_yes], layout=layout)\n\n# Show the figure\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-16T19:04:20.891338Z","iopub.execute_input":"2023-10-16T19:04:20.891749Z","iopub.status.idle":"2023-10-16T19:04:21.187487Z","shell.execute_reply.started":"2023-10-16T19:04:20.891717Z","shell.execute_reply":"2023-10-16T19:04:21.186691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Similar visualization for MonthlyCharges\n\n# Filter data\nmc_churn_no = df[df.Churn == 'No'].MonthlyCharges\nmc_churn_yes = df[df.Churn == 'Yes'].MonthlyCharges\n\n\n\n\n# Create histogram traces with the specified number of bins\ntrace_mc_churn_no = go.Histogram(\n    x=mc_churn_no,\n    opacity=0.5,\n    marker=dict(color='green'),\n    name='Churn=No',\n    nbinsx=10  # Set the number of bins here\n)\n\ntrace_mc_churn_yes = go.Histogram(\n    x=mc_churn_yes,\n    opacity=0.5,\n    marker=dict(color='red'),\n    name='Churn=Yes',\n    nbinsx=10  # Set the number of bins here\n)\n\n# Create layout with adjusted bar width\nlayout = go.Layout(\n    title=\"Customer Churn Prediction Visualization\",\n    xaxis=dict(title=\"Monthly Charges\"),\n    yaxis=dict(title=\"Number Of Customers\"),\n    barmode='group',\n    bargap=0.4,  # Calculate bargap based on bar width\n    bargroupgap=0.1,\n)\n\n# Create figure\nfig = go.Figure(data=[trace_mc_churn_no, trace_mc_churn_yes], layout=layout)\n\n# Show the figure\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-16T19:04:21.303443Z","iopub.execute_input":"2023-10-16T19:04:21.304544Z","iopub.status.idle":"2023-10-16T19:04:21.329083Z","shell.execute_reply.started":"2023-10-16T19:04:21.304496Z","shell.execute_reply":"2023-10-16T19:04:21.327705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Define a function to print unique values in object-type columns              \ndef print_unique_col_values(df):\n    for column, dtype in df.select_dtypes(include=['object']).items():\n        unique_values = df[column].unique()\n        print(f'{column}: {unique_values}')                \n\n# Print unique values for object-type columns\nprint_unique_col_values(df)                \n                ","metadata":{"execution":{"iopub.status.busy":"2023-10-16T19:04:25.439847Z","iopub.execute_input":"2023-10-16T19:04:25.440593Z","iopub.status.idle":"2023-10-16T19:04:25.464877Z","shell.execute_reply.started":"2023-10-16T19:04:25.440555Z","shell.execute_reply":"2023-10-16T19:04:25.464079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In this part, we define a data preprocessing pipeline using scikit-learn's Pipeline\n# and ColumnTransformer to prepare data for machine learning.\n\n\n# Define columns with binary 'Yes'/'No' values\nyes_no_columns = ['Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup',\n                  'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling']\n\n# Define functions to replace values like \"No internet service\" and \"No phone service\" with simply \"No\" \n# in the specified columns. This is done to standardize the binary values.\ndef replace_yes_no_values(column):\n    column.replace(\"No internet service\", \"No\", inplace=True)\n    column.replace(\"No phone service\", \"No\", inplace=True)\n    return column\n\n# To encode the binary 'Yes'/'No' values into numerical values\ndef encode_yes_no_values(column):\n    return column.replace({'Yes': 1, 'No': 0})\n\n# Create transformer contain above functions\nyes_no_transformer = Pipeline([\n    ('replace_yes_no_values', FunctionTransformer(func=replace_yes_no_values)),\n    ('encode_yes_no', FunctionTransformer(func=encode_yes_no_values))\n])\n\n# Create a pipeline for one-hot encoding categorical columns\ncategorical_transformer = Pipeline([\n    ('one_hot_encode', OneHotEncoder())\n])\n\n# Create a pipeline for scaling numeric columns using Min-Max scaling\nnumeric_transformer = Pipeline([\n    ('scale_features', MinMaxScaler())\n])\n\n# Create a column transformer to apply different transformers to specific columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('yes_no', yes_no_transformer, yes_no_columns),\n        ('categorical', categorical_transformer, ['gender', 'Contract', 'PaymentMethod', 'InternetService']),\n        ('numeric', numeric_transformer, ['tenure', 'MonthlyCharges', 'TotalCharges'])\n    ],\n    remainder='passthrough'  # Include columns not specified in transformers\n)\n\n# Create the final data processing pipeline\ndata_processing_pipeline = Pipeline([\n    ('preprocessor', preprocessor)\n])\n\n\n\n\n\n# Save the pipeline\ndef preprocess_and_save_pipeline(df):\n    # Separate features (X) and target (y)\n    X = df.drop('Churn', axis='columns')\n    y = df['Churn'].map({'Yes': 1, 'No': 0})\n\n    # Split the data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n\n    # Fit the data processing pipeline to the training data\n    X_train = data_processing_pipeline.fit_transform(X_train)\n\n    # Save the pipeline to a file\n    joblib.dump(data_processing_pipeline, 'preprocessor.pkl')\n\n\n\npreprocess_and_save_pipeline(df)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T19:04:28.679173Z","iopub.execute_input":"2023-10-16T19:04:28.679618Z","iopub.status.idle":"2023-10-16T19:04:28.759153Z","shell.execute_reply.started":"2023-10-16T19:04:28.679585Z","shell.execute_reply":"2023-10-16T19:04:28.758133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Model Selection and Hyperparameter Tuning\n","metadata":{}},{"cell_type":"markdown","source":"**Handling Imbalanced Datasets with Class Weights**\n\nWe know that, the dataset is imbalanced, meaning that one class (\"Churn: Yes\") has significantly fewer samples than the other class (\"Churn: No\"). Handling imbalanced datasets is crucial because it can lead to biased models that perform poorly on minority classes. Traditionally, techniques such as undersampling (reducing the majority class samples), oversampling (increasing the minority class samples), or using Synthetic Minority Over-sampling Technique (SMOTE) are employed to balance the dataset.\n\nHowever, instead of using these traditional methods, this code employs a different approach: using class weights. The class weights are assigned to different classes in the dataset to give higher importance to the minority class during model training. This method has some advantages over traditional techniques:\n\n**No Data Modification:** Unlike undersampling and oversampling, which involve modifying the dataset by removing or duplicating samples, class weights do not require any changes to the original data.\n\n**Less Risk of Overfitting:** Oversampling can lead to overfitting, especially when the minority class is small. Using class weights does not introduce additional data, reducing the risk of overfitting.\n\n**Preserves Information:** Undersampling can lead to a loss of information by discarding samples. Class weights allow all data to be used in training.","metadata":{}},{"cell_type":"code","source":"#Here we perform a grid search for hyperparameter tuning on several machine learning models and\n# evaluate their performance using different scoring metrics.\n\n\n\n\n# Function for grid search on a model with specified parameters\ndef grid_search_model(model, params, X_train, y_train, scorer):\n    \"\"\"\n    This function performs a grid search on the given model with the provided parameters.\n    \n    \n    Returns:\n        best_estimator: The best model after grid search.\n        best_params: The best hyperparameters found during grid search.\n    \"\"\"\n    # Initialize GridSearch\n    gs = GridSearchCV(model, params, scoring=scorer, cv=5, n_jobs=-1)\n    \n    # Fit the model\n    gs.fit(X_train, y_train)\n    \n    # Return the best estimator and its best parameters\n    return gs.best_estimator_, gs.best_params_\n\n\n# Define a function for evaluating various metrics on a model\ndef evaluate_metrics(model, X_train, y_train, X_test, y_test):\n    \n    \n    \"\"\"\n    This function evaluates the given model on multiple metrics for both training and test datasets.\n\n    Returns:\n        metrics: A dictionary containing various evaluation metrics for training and test datasets.\n    \"\"\"\n    \n    \n    metrics = {}\n\n    for dataset, X, y in [('train', X_train, y_train), ('test', X_test, y_test)]:\n        y_pred = model.predict(X)\n        y_prob = model.predict_proba(X)[:, 1]\n        acc = accuracy_score(y, y_pred)\n        recall = recall_score(y, y_pred)\n        f1 = f1_score(y, y_pred)\n        auc = roc_auc_score(y, y_prob)\n\n        metrics[f'{dataset}_ACC'] = acc\n        metrics[f'{dataset}_recall'] = recall\n        metrics[f'{dataset}_f1_score'] = f1\n        metrics[f'{dataset}_AUC'] = auc\n\n    return metrics\n\n\n\n# Define the main function for grid search\ndef main_grid_search(df):\n    \n    \"\"\"\n    This is the main function that performs grid search and model evaluation.\n    Args:\n        df: The input DataFrame containing features and labels.\n    Returns:\n        all_results: A dictionary containing evaluation results for different models and scoring metrics.\n    \"\"\"\n    \n    X = df.drop('Churn',axis='columns')\n    y = df['Churn'].map({'Yes': 1, 'No': 0})\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n    \n    # Fit the pipeline to your data\n    X_train = data_processing_pipeline.fit_transform(X_train)\n    X_test = data_processing_pipeline.transform(X_test)\n    \n    \n    # Calculate the class weight to handle class imbalance\n    weight = sum(y_train == 0) / sum(y_train == 1)\n    \n    # Define a dictionary of model names and their parameter grids\n    models_and_parameters = {\n        \"LogisticRegression\": {\n            'model': LogisticRegression(),\n            'params': {\n                'C': [0.001, 0.01, 0.1, 1, 10, 100],\n                'class_weight': [{0: 1, 1: weight}],\n                'penalty': ['l1', 'l2'],\n                'solver': ['liblinear', 'saga']\n            }\n        },\n        \"RandomForestClassifier\": {\n            'model': RandomForestClassifier(),\n            'params': {\n                'n_estimators': [100, 300, 500],\n                'max_depth': [None,2, 5, 10, 20, 30],\n                'class_weight': [{0: 1, 1: weight}],\n                'min_samples_split': [2, 5, 10],\n                'min_samples_leaf': [1, 2, 4]\n            }\n        },\n        \"XGBClassifier\": {\n            'model': XGBClassifier(),\n            'params': {\n                'n_estimators': [100, 300, 500],\n                'learning_rate': [0.01, 0.1, 0.5],\n                'scale_pos_weight': [weight],\n                'max_depth': [2, 3, 4, 5, 6],\n                'subsample': [0.8, 0.9, 1.0],\n                'colsample_bytree': [0.8, 0.9, 1.0]\n            }\n        },\n        \"CatBoostClassifier\": {\n            'model': CatBoostClassifier(verbose=False),\n            'params': {\n                'iterations': [100, 300, 500],\n                'learning_rate': [0.01, 0.1, 0.5],\n                'class_weights': [[1, weight]],\n                'depth': [2,4, 6, 8],\n                'l2_leaf_reg': [1, 3, 5, 7]\n            }\n        },\n        \"LGBMClassifier\": {\n            'model': LGBMClassifier(verbose=-1),\n            'params': {\n                'n_estimators': [100, 300, 500],\n                'learning_rate': [0.01, 0.1, 0.5],\n                'class_weight': [{0: 1, 1: weight}],\n                'max_depth': [2, 3, 4, 5, 6],\n                'min_child_samples': [5, 10, 20]\n            }\n        }\n    }\n\n    # Define the scorers\n    scorers = {\n        'recall': make_scorer(recall_score),\n        'f1': make_scorer(f1_score),\n        'roc_auc': make_scorer(roc_auc_score, greater_is_better=True, needs_proba=True)\n    }\n    \n    \n    \n    # Create a master dictionary to store results for all scorers\n    all_results = {}\n\n    for scorer_name, scorer in scorers.items():\n        scorer_results = {}\n\n        for model_name, model_params in tqdm(models_and_parameters.items(), desc=f'Grid Searching models using {scorer_name}'):\n            best_model, best_params = grid_search_model(model_params['model'], model_params['params'], X_train, y_train, scorer)\n\n            # Evaluate metrics on the training and test set\n            metrics = evaluate_metrics(best_model, X_train, y_train, X_test, y_test)\n            metrics['best_params'] = best_params\n\n            scorer_results[model_name] = metrics\n\n        \n\n        # Save results for the current scorer to the master dictionary\n        all_results[scorer_name] = scorer_results\n        print(all_results)\n\n    return all_results","metadata":{"execution":{"iopub.status.busy":"2023-10-16T10:41:55.76895Z","iopub.execute_input":"2023-10-16T10:41:55.769288Z","iopub.status.idle":"2023-10-16T10:41:55.787132Z","shell.execute_reply.started":"2023-10-16T10:41:55.769264Z","shell.execute_reply":"2023-10-16T10:41:55.78583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform grid search and model evaluation on the provided DataFrame\nFull_grid_result = main_grid_search(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As you know, grid search can be quite time-consuming.\n# We can speed it up by checking fewer options or using a randomized grid search.\n# However, for the sake of experimentation, I'm using a standard grid search in this code, which takes more than three hours to complete.\n\n# This part of the code saves the results of the grid search.\n# so that we don't have to run the grid search again in the future. This way, we save time and effort.\n\n\n\n# To save the output dictionary, we can use joblib\njoblib.dump(Full_grid_result, 'Full_grid_result.pkl')\n\n# To load the saved dictionary, we  can use this code:\n# loaded_data_dict = joblib.load('Full_grid_result.pkl')","metadata":{"execution":{"iopub.status.busy":"2023-10-16T17:18:11.871631Z","iopub.execute_input":"2023-10-16T17:18:11.872025Z","iopub.status.idle":"2023-10-16T17:18:11.881267Z","shell.execute_reply.started":"2023-10-16T17:18:11.871999Z","shell.execute_reply":"2023-10-16T17:18:11.880623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Model Evaluation, Comparison and Visualization","metadata":{}},{"cell_type":"code","source":"# This visualization is derived from the dictionary obtained by executing the main_grid_search() function.\n\n# Define a function to plot and compare model metrics\ndef plot_comparison(data, scorer):\n    models = list(data[scorer].keys())\n    train_acc = [data[scorer][model]['train_ACC'] for model in models]\n    test_acc = [data[scorer][model]['test_ACC'] for model in models]\n    train_recall = [data[scorer][model]['train_recall'] for model in models]\n    test_recall = [data[scorer][model]['test_recall'] for model in models]\n    train_f1 = [data[scorer][model]['train_f1_score'] for model in models]\n    test_f1 = [data[scorer][model]['test_f1_score'] for model in models]\n    train_auc = [data[scorer][model]['train_AUC'] for model in models]\n    test_auc = [data[scorer][model]['test_AUC'] for model in models]\n\n    fig = go.Figure()\n\n    fig.add_trace(go.Scatter(x=models, y=train_acc, mode='lines+markers', name='Train ACC'))\n    fig.add_trace(go.Scatter(x=models, y=test_acc, mode='lines+markers', name='Test ACC'))\n\n    fig.add_trace(go.Scatter(x=models, y=train_recall, mode='lines+markers', name='Train Recall'))\n    fig.add_trace(go.Scatter(x=models, y=test_recall, mode='lines+markers', name='Test Recall'))\n\n    fig.add_trace(go.Scatter(x=models, y=train_f1, mode='lines+markers', name='Train F1 Score'))\n    fig.add_trace(go.Scatter(x=models, y=test_f1, mode='lines+markers', name='Test F1 Score'))\n\n    fig.add_trace(go.Scatter(x=models, y=train_auc, mode='lines+markers', name='Train AUC'))\n    fig.add_trace(go.Scatter(x=models, y=test_auc, mode='lines+markers', name='Test AUC'))\n\n    fig.update_xaxes(type='category', tickangle=45)\n    fig.update_layout(title=f'{scorer} - Train and Test Metrics Comparison',\n                      xaxis_title='Models',\n                      yaxis_title='Metric Value')\n    # Set y-axis range from 0.5 to 1\n    fig.update_yaxes(range=[0.5, 1])\n    \n    fig.show()\n\n# Call the function for each scorer\ndata = Full_grid_result.copy()\nfor scorer in ['recall', 'f1', 'roc_auc']:\n    plot_comparison(data, scorer)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T14:50:32.513549Z","iopub.execute_input":"2023-10-16T14:50:32.513937Z","iopub.status.idle":"2023-10-16T14:50:32.561918Z","shell.execute_reply.started":"2023-10-16T14:50:32.513909Z","shell.execute_reply":"2023-10-16T14:50:32.56117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Model Selection and Recommendation","metadata":{}},{"cell_type":"markdown","source":"We are grappling with the challenge of customer churn. In these scenarios, our main objective is to identify the positive class, which represents customers who intend to churn. Accurately predicting churn is critical. It's more acceptable to incorrectly label a loyal customer as churning (a false positive) than to overlook a customer who is genuinely planning to churn (a false negative).\n\nGiven the context, we have two primary approaches:\n\n1- Prioritize Recall: In this approach, we emphasize recall, which measures the model's proficiency in detecting customers who are genuinely intending to churn. A high recall may come at the cost of a higher false positive rate, meaning we might wrongly identify some loyal customers as churning.\n\n2 - Balanced Trade-off: The second approach aims to achieve a balance. We try to find a model that excels in recall and also maintains decent accuracy to reduce the number of false identifications.\n\nFor the first approach, models like XGBoost and LightGBM have shown promise.\nFor the second, balanced approach, CatBoost seems to strike an effective compromise between the two metrics.\n\nI favor the CatBoost model because it provides a comprehensive and generalized solution.\nGiven its outstanding performance across various metrics, let's determine the best CatBoost model by examining the hyperparameters.","metadata":{}},{"cell_type":"code","source":"# In this analysis, we aim to compare CatBoost classifiers that were developed using different scoring metrics.\n# The goal is to select the best-performing model to save for future use.\n# The visualizations and comparisons are generated from the dictionary obtained through the execution of the main_grid_search() function.\n\ndata = Full_grid_result.copy()\n\n# Extract CatBoostClassifier results for each scorer\ncatboost_data = {scorer: data[scorer]['CatBoostClassifier'] for scorer in data}\n\n# Define metrics to compare\nmetrics = ['train_ACC', 'train_recall', 'train_f1_score', 'train_AUC', \n           'test_ACC', 'test_recall', 'test_f1_score', 'test_AUC']\n\n# Prepare data for plotting\nplot_data = {}\nfor scorer in catboost_data:\n    for metric in metrics:\n        plot_data.setdefault(metric, []).append(catboost_data[scorer][metric])\n\n# Create bar chart\nfig = go.Figure()\n\nfor metric, values in plot_data.items():\n    fig.add_trace(go.Bar(\n        x=list(catboost_data.keys()),\n        y=values,\n        name=metric\n    ))\n\nfig.update_layout(\n    barmode='group',\n    title=\"CatBoostClassifier Metrics Comparison Across Scorers\",\n    xaxis_title=\"Scorer\",\n    yaxis_title=\"Value\",\n    legend_title=\"Metric\"\n)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-16T15:17:19.99522Z","iopub.execute_input":"2023-10-16T15:17:19.995558Z","iopub.status.idle":"2023-10-16T15:17:20.034924Z","shell.execute_reply.started":"2023-10-16T15:17:19.995534Z","shell.execute_reply":"2023-10-16T15:17:20.033926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis:**\n\n* The CatBoostClassifier optimized for Recall has the highest recall, as expected. However, its F1 Score and AUC are the lowest among the three.\n\n* The CatBoostClassifier optimized for F1 Score has a better balance between precision and recall, leading to the second-highest F1 Score. It also has a slightly higher AUC than the one optimized for recall.\n\n* The CatBoostClassifier optimized for AUC has the highest F1 Score and AUC among the three. Its recall, while not the highest, is still reasonably high.\n\n**Recommendation:**\n\nConsidering the balance across all metrics, the CatBoostClassifier optimized for AUC seems to be the most preferred. It has the highest F1 Score and AUC and maintains a good recall, indicating its capability to perform well in both identifying the positive class and distinguishing between the two classes.","metadata":{}},{"cell_type":"code","source":"# Extract the best hyperparameters for the chosen CatBoostClassifier.\nbest_params_catboost = Full_grid_result['roc_auc']['CatBoostClassifier']['best_params']\nprint(best_params_catboost)","metadata":{"execution":{"iopub.status.busy":"2023-10-16T15:26:17.261039Z","iopub.execute_input":"2023-10-16T15:26:17.261532Z","iopub.status.idle":"2023-10-16T15:26:17.267377Z","shell.execute_reply.started":"2023-10-16T15:26:17.261491Z","shell.execute_reply":"2023-10-16T15:26:17.266358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Displaying the Outcomes of the Chosen Model While Preserving the Model Itself","metadata":{}},{"cell_type":"code","source":"# Define a function to evaluate model, including the selected CatBoostClassifier\ndef evaluate_models(models, X_train, y_train, X_test, y_test):\n    results = {}\n    \n    for model in models:\n        model_name = type(model).__name__\n        results[model_name] = {}\n        \n        # Train the model\n        model.fit(X_train, y_train)\n        \n        # Predictions\n        y_train_pred = model.predict(X_train)\n        y_test_pred = model.predict(X_test)\n        \n        # Get the probabilities for ROC AUC\n        if hasattr(model, \"predict_proba\"):\n            y_train_prob = model.predict_proba(X_train)[:, 1]\n            y_test_prob = model.predict_proba(X_test)[:, 1]\n        else:\n            y_train_prob = y_train_pred  # For models that don't have predict_proba\n            y_test_prob = y_test_pred\n        \n        # Metrics for training data\n        results[model_name]['train_accuracy'] = accuracy_score(y_train, y_train_pred)\n        results[model_name]['train_precision'] = precision_score(y_train, y_train_pred)\n        results[model_name]['train_recall'] = recall_score(y_train, y_train_pred)\n        results[model_name]['train_f1'] = f1_score(y_train, y_train_pred)\n        results[model_name]['train_roc_auc'] = roc_auc_score(y_train, y_train_prob)\n        \n        # Metrics for test data\n        results[model_name]['test_accuracy'] = accuracy_score(y_test, y_test_pred)\n        results[model_name]['test_precision'] = precision_score(y_test, y_test_pred)\n        results[model_name]['test_recall'] = recall_score(y_test, y_test_pred)\n        results[model_name]['test_f1'] = f1_score(y_test, y_test_pred)\n        results[model_name]['test_roc_auc'] = roc_auc_score(y_test, y_test_prob)\n        \n    return results\n\n\n\n# Define the main function to train the model from scratch\ndef main_evaluation(df):\n    \n    X = df.drop('Churn',axis='columns')\n    y = df['Churn'].map({'Yes': 1, 'No': 0})\n\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n    \n    # Fit the pipeline to your data\n    X_train = data_processing_pipeline.fit_transform(X_train)\n\n    X_test = data_processing_pipeline.transform(X_test)\n    \n    weight = sum(y_train == 0) / sum(y_train == 1)\n    \n    models = [\n\n        CatBoostClassifier(class_weights=[1, weight],  depth = 2, iterations =  100, l2_leaf_reg = 5, learning_rate = 0.1, verbose = False) \n\n    ]\n    \n    for model in tqdm(models, desc='Evaluating models'):\n        results = evaluate_models(models, X_train, y_train, X_test, y_test)\n        joblib.dump(model, 'catboost_model.joblib')\n    return results    \n\n\n\n# Perform model evaluation with the selected CatBoostClassifier\ncatboost_evaluated = main_evaluation(df)\ncatboost_evaluated","metadata":{"execution":{"iopub.status.busy":"2023-10-16T16:18:44.434571Z","iopub.execute_input":"2023-10-16T16:18:44.43496Z","iopub.status.idle":"2023-10-16T16:18:44.979269Z","shell.execute_reply.started":"2023-10-16T16:18:44.434927Z","shell.execute_reply":"2023-10-16T16:18:44.97829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}